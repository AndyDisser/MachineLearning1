{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "240d4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Name - Matrikelnummer \n",
    "1) Pham, Ngoc Anh Trung - 7176267\n",
    "2) Viktor Vironski - 4330455\n",
    "3) Andy Disser - 5984875\n",
    "\n",
    "Exercise Sheet 6\n",
    "\"\"\"\n",
    "\n",
    "from random import random\n",
    "from sklearn import cluster, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Node of a decision tree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_numerical, num_value, variable_index):\n",
    "        self.left_child = None  # left child\n",
    "        self.right_child = None  # right child\n",
    "        self.is_numerical = is_numerical   # is numerical or not (i.e. is it a comparison of numerical)\n",
    "        self.num_value = num_value  # numerical value (the threshold to be compared)\n",
    "        self.variable_index = variable_index  # the index of the variable needs to be compared\n",
    "        self.output_val = None  # The output value of this leaf for classification\n",
    "\n",
    "\n",
    "def compute_gini_impurity(arr, labels, val):\n",
    "    \"\"\"\n",
    "    Auxillary function for computing gini impurity value of the given array\n",
    "    when using element with value val as root (numerical comparison and not binary comparison)\n",
    "\n",
    "    Inputs:\n",
    "    - arr : the given array\n",
    "    - val : the given value\n",
    "    - labels : the given labels\n",
    "    Output:\n",
    "    - the gini node impurity when using val as comparison value\n",
    "    \"\"\"\n",
    "\n",
    "    # Only proceed if both the value is not nan and no entry in arr is nan\n",
    "    if not (np.isinf(val)) and not (np.isinf(arr).any()):\n",
    "        n = np.size(arr)\n",
    "        left = arr[arr <= val]\n",
    "        right = arr[arr > val]\n",
    "        left_labels = labels[arr <= val]\n",
    "        right_labels = labels[arr > val]\n",
    "\n",
    "        # Yes : it belongs to class 0, No: it not belongs to class 0\n",
    "        l = np.size(left)\n",
    "        left_yes = np.size(left_labels[left_labels == 0])\n",
    "        left_no = l - left_yes\n",
    "        \n",
    "        r = np.size(right)\n",
    "        right_yes = np.size(right_labels[right_labels == 0])\n",
    "        right_no = r - right_yes\n",
    "\n",
    "        #1 - p(yes)**2 - p(no)**2\n",
    "        left_impurity = 1 - np.power(np.divide(left_yes, l), 2) - np.power(np.divide(left_no, l), 2)\n",
    "\n",
    "        if r != 0:\n",
    "            # 1 - p(left)**2 - p(right)**2\n",
    "            right_impurity = 1 - np.power(np.divide(right_yes, r), 2) - np.power(np.divide(right_no, r), 2)\n",
    "        else:\n",
    "            right_impurity = 0\n",
    "\n",
    "        # node impurity : (#left/ #(left+right))*left_impurity + (#right/ #(left+right))*right_impurity\n",
    "        node_impurity = np.multiply(np.divide(l, l+r), left_impurity) + np.multiply(np.divide(r, l+r), right_impurity)\n",
    "\n",
    "        return node_impurity\n",
    "\n",
    "    return np.infty\n",
    "\n",
    "\n",
    "def decisiontree(data, labels, gini_impurity):\n",
    "    \"\"\"\n",
    "    Build a decision tree based on the given data and labels\n",
    "\n",
    "    Inputs:\n",
    "    data : the given data\n",
    "    labels : it's labels\n",
    "    gini_impurity: initial gini impurity, usually inf\n",
    "\n",
    "    Output:\n",
    "    the root node of the decision tree\n",
    "    \"\"\"\n",
    "    \n",
    "    m = data.shape[1]  # the numbers of columns/ variables\n",
    "    n = data.shape[0]  # the numbers of samples\n",
    "    \n",
    "    # transpose, for the sake of computing\n",
    "    temp_data = np.transpose(data)\n",
    "\n",
    "    # sort each row\n",
    "    sorted_temp_data = np.sort(temp_data, axis=1)\n",
    "    \n",
    "    # Average of each adjacent pairs\n",
    "    if n > 1:\n",
    "        threshold_candidates = np.divide(np.delete(sorted_temp_data, n-1, axis=1)\\\n",
    "            +np.delete(sorted_temp_data, 0, axis=1), 2)\n",
    "    else:\n",
    "        threshold_candidates = sorted_temp_data\n",
    "\n",
    "    # sort and take the sort indices each row of this tranposed matrix\n",
    "    sort_indices = np.argsort(temp_data, axis=1)\n",
    "\n",
    "    # permutate the labels (before that broadcast it to (m x n) matrix) accordingly\n",
    "    # each row is a permuation of the labels according to the sorting of that row\n",
    "    sorted_labels = np.take_along_axis(np.broadcast_to(labels[np.newaxis, :], (m, n)), sort_indices, 1)\n",
    "\n",
    "    # Vectorize the function of computing gini impurity\n",
    "    vec_compute_gini_impurity = np.vectorize(compute_gini_impurity, signature='(m), (m), () -> ()')\n",
    "\n",
    "    # Impurity matrix , each entry (i, j) representing the gini impurity when using \n",
    "    # the entry (i, j) in sorted_temp_data as cutoff node.\n",
    "    impurity_matrix = vec_compute_gini_impurity(sorted_temp_data[:, np.newaxis, :],\\\n",
    "        sorted_labels[:, np.newaxis, :], threshold_candidates)\n",
    "\n",
    "    # lowest possible gini impurity of any sample and any variable\n",
    "    min_gini_impurity = np.nanmin(np.apply_over_axes(np.nanmin, impurity_matrix, 1).reshape(m))\n",
    "\n",
    "    # Variable index/row index that will be used to cut off\n",
    "    var_index = np.nanargmin(np.apply_over_axes(np.nanmin, impurity_matrix, 1).reshape(m))\n",
    "\n",
    "    #column index/ the index of the sample having the least impurity in this variable index\n",
    "    samp_index = np.apply_over_axes(np.argmin, impurity_matrix, 1).reshape(m)[var_index]\n",
    "\n",
    "    # The value of the cutoff node\n",
    "    min_val = sorted_temp_data[var_index, samp_index]\n",
    "\n",
    "    # Create a node containing the comparison\n",
    "    node = Node(True, min_val, var_index)\n",
    "\n",
    "    # Get the value of this specific variable\n",
    "    min_data = data[:, var_index]\n",
    "\n",
    "    # Put the samples than to the according left or right subtree\n",
    "    left = data[min_data <= min_val]\n",
    "    right = data[min_data > min_val]\n",
    "    left_labels = labels[min_data <= min_val]\n",
    "    right_labels = labels[min_data > min_val]\n",
    "\n",
    "    # fill the already processed variable as infinity, causing it irrelevant for future computation\n",
    "    left[:, var_index] = np.infty\n",
    "    right[:, var_index] = np.infty\n",
    "\n",
    "    if (np.all(np.isinf(data))) or (min_gini_impurity >= gini_impurity):\n",
    "        # Base case, if the data only have inf left or\n",
    "        # the gini impurity can not be any better\n",
    "\n",
    "        # Then this node is a leaf, calculate the output value of this leaf based\n",
    "        # on the the class that is more dominant in this leaf       \n",
    "        if np.size(labels[labels == 0]) >= np.size(labels[labels == 1]):\n",
    "            node.output_val = 0\n",
    "        else:\n",
    "            node.output_val = 1\n",
    "\n",
    "        return node\n",
    "\n",
    "    else:  # General case\n",
    "\n",
    "        # Recurse for left and right subtree if there is any sample being assigned there\n",
    "        if np.size(left) > 0:\n",
    "            left_recurse = decisiontree(left, left_labels, min_gini_impurity)\n",
    "            node.left_child = left_recurse  # Set the left child\n",
    "        if np.size(right) > 0:\n",
    "            right_recurse = decisiontree(right, right_labels, min_gini_impurity)\n",
    "            node.right_child = right_recurse  # Set the right child\n",
    "\n",
    "        return node\n",
    "    \n",
    "def predict(root, data):\n",
    "    \"\"\"\n",
    "    Predict using the a decision tree\n",
    "\n",
    "    data: the given data\n",
    "    root: the root node of the given decision tree\n",
    "\n",
    "    Output:\n",
    "    0 or 1, 0 being the sample belongs to class 0, 1 being the sample not belong to class 0\n",
    "    \"\"\"\n",
    "\n",
    "    curr_node = root  # current node\n",
    "\n",
    "    # while it is not a leaf, has atleast one child\n",
    "    while curr_node.left_child is not None or curr_node.right_child is not None:\n",
    "        \n",
    "        var_index = curr_node.variable_index\n",
    "        \n",
    "        # Left (\"<=\"): the sample belongs to class 0, right (\">\"): the sample belongs to class 1 (not 0)\n",
    "        if curr_node.left_child is not None:\n",
    "            if curr_node.right_child is None:  # has left but doesn't have any right child\n",
    "                curr_node = curr_node.left_child\n",
    "            else:  # has both children\n",
    "                if data[var_index] <= curr_node.num_value:\n",
    "                    curr_node = curr_node.left_child\n",
    "                else:  # data[var_index] > curr_node.num_value\n",
    "                    curr_node = curr_node.right_child\n",
    "        else:  # doesn't have any left child, but has a right child\n",
    "            curr_node = curr_node.right_child\n",
    "\n",
    "    return curr_node.output_val\n",
    "\n",
    "\n",
    "def bootstrapdataset(data, nsamples):\n",
    "    \"\"\"\n",
    "    Build bootstrapped dataset\n",
    "\n",
    "    Inputs:\n",
    "    - data: the given data\n",
    "    - nsamples: the number of random samples with possible duplicate\n",
    "\n",
    "    Output:\n",
    "    - the bootstrapped dataset of the random samples with a subset of 3 features\n",
    "    - the random indices with possbile duplicate\n",
    "    \"\"\"\n",
    "\n",
    "    m = data.shape[1]  # the numbers of columns/ variables\n",
    "    n = data.shape[0]  # the numbers of samples\n",
    "\n",
    "    if nsamples == -1:\n",
    "        rand_sample_indices = np.random.choice(np.arange(n), n, replace=True)\n",
    "    else:\n",
    "        rand_sample_indices = np.random.choice(np.arange(n), nsamples, replace=True)\n",
    "\n",
    "    # Define the number of subset to be used\n",
    "    num_of_features = 3\n",
    "\n",
    "    # Only use this subset of the features\n",
    "    features_subset = np.random.choice(np.arange(m), num_of_features, replace=False)\n",
    "\n",
    "    # Filter the data based on the random indices and the subset of features\n",
    "    data = data[:, features_subset]\n",
    "    data = data[rand_sample_indices, :]\n",
    "\n",
    "    return data, rand_sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bd769b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. run, Accuracy: 0.9622641509433962\n",
      "2. run, Accuracy: 0.9433962264150944\n",
      "3. run, Accuracy: 0.9245283018867925\n",
      "4. run, Accuracy: 0.8490566037735849\n",
      "5. run, Accuracy: 0.9056603773584906\n"
     ]
    }
   ],
   "source": [
    "# Import and access  data\n",
    "wein = datasets.load_wine()\n",
    "# print(wein.data.shape)  # 178, 13\n",
    "\n",
    "# ==================== Exercise 1a : repeat 5 times ========================:\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    # Create permutation of 0,1,..,177\n",
    "    permutation = np.random.permutation(np.arange(178))\n",
    "    train_filter = permutation < 125\n",
    "    test_filter = permutation >= 125 \n",
    "\n",
    "    # Split the data, 125 samples (70%) for training, 53 samples (30%) for testing\n",
    "    train_data = wein.data[train_filter]\n",
    "    test_data = wein.data[test_filter]\n",
    "\n",
    "    # The respective labels\n",
    "    train_labels = wein.target[train_filter]\n",
    "    test_labels = wein.target[test_filter]\n",
    "\n",
    "    # merge classes bigger than 0 to 1 -> seperate 0 and not 0\n",
    "    train_labels[train_labels > 0] = 1\n",
    "    test_labels[test_labels > 0] = 1\n",
    "\n",
    "    root = decisiontree(train_data, train_labels, np.infty)\n",
    "\n",
    "    # vectorize and predict\n",
    "    vec_predict = np.vectorize(predict, excluded='root', signature='(), (m) -> ()')\n",
    "    predicted_labels = vec_predict(root, test_data)\n",
    "\n",
    "    #print(\"predicted_labels:\", predicted_labels)\n",
    "    #print(\"truth labels:\", test_labels)\n",
    "\n",
    "    accuracy = np.sum(predicted_labels == test_labels) / np.size(test_labels)\n",
    "\n",
    "    print(f'{_+1}. run, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c7cb679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest with 10 trees, Accuracy: 0.6226415094339622\n",
      "Forest with 25 trees, Accuracy: 0.6037735849056604\n",
      "Forest with 100 trees, Accuracy: 0.5849056603773585\n"
     ]
    }
   ],
   "source": [
    "# ==================== Exercise 1b : build random forests ========================:\n",
    "\n",
    "# Create permutation of 0,1,..,177\n",
    "permutation = np.random.permutation(np.arange(178))\n",
    "train_filter = permutation < 125\n",
    "test_filter = permutation >= 125 \n",
    "\n",
    "# Split the data, 125 samples (70%) for training, 53 samples (30%) for testing\n",
    "train_data = wein.data[train_filter]\n",
    "test_data = wein.data[test_filter]\n",
    "\n",
    "# The respective labels\n",
    "train_labels = wein.target[train_filter]\n",
    "test_labels = wein.target[test_filter]\n",
    "\n",
    "# merge classes bigger than 0 to 1 -> seperate 0 and not 0\n",
    "train_labels[train_labels > 0] = 1\n",
    "test_labels[test_labels > 0] = 1    \n",
    "\n",
    "n = train_data.shape[0]   # Number of samples in train data\n",
    "m = train_data.shape[1]   # number of features/variables in train data\n",
    "\n",
    "num_of_trees = [10, 25, 100]\n",
    "\n",
    "for k in num_of_trees:\n",
    "\n",
    "    # Vetorize the bootstrapdataset function\n",
    "    vec_bootstrapdataset= np.vectorize(bootstrapdataset, excluded='nsamples', signature='(m, n), () -> (k, l), (o)')\n",
    "\n",
    "    # Broadcast the traindata and apply the vectorized bootstrapdataset on it\n",
    "    broadcasted_train_data = np.broadcast_to(train_data, (k, n, m))\n",
    "    bs_datasets, bs_random_indices = vec_bootstrapdataset(broadcasted_train_data, -1)\n",
    "\n",
    "    # Extract the according labels\n",
    "    broadcasted_train_labels = np.broadcast_to(train_labels, (k, n))\n",
    "    bs_labels = np.take_along_axis(broadcasted_train_labels, bs_random_indices, axis=1)\n",
    "\n",
    "    # build the forest\n",
    "    vec_decisiontree = np.vectorize(decisiontree, excluded='gini_impurity', signature='(n, m), (n), () -> ()')\n",
    "    forest = vec_decisiontree(bs_datasets, bs_labels, np.infty)\n",
    "\n",
    "    # use the forest to predict, before that vectorize the predict function accordingly\n",
    "    vec_predict = np.vectorize(predict, signature='(), (m) -> ()')  \n",
    "    result = vec_predict(forest[np.newaxis, :], test_data[:, np.newaxis, :])\n",
    "\n",
    "    # Sum over the result of each tree in forest, based on majority\n",
    "    # if the sum (the number of 1) larger than 0, then the algorithm will\n",
    "    # decide for 1 and vice versa\n",
    "    mysum = np.apply_over_axes(np.sum, result, 1).reshape(test_data.shape[0])\n",
    "    predicted = (mysum >= (k//2+1)).astype(int)\n",
    "\n",
    "    # The final accuracy / Build the forest with 100 trees can take longer based on the size of the subset of features.\n",
    "    accuracy = np.sum(predicted == test_labels) / np.size(test_labels)\n",
    "    print(f'Forest with {k} trees, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89845d",
   "metadata": {},
   "source": [
    "# Auf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce2ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapdataset(data, nsamples, weights=None):\n",
    "    \"\"\"\n",
    "    Build bootstrapped dataset\n",
    "\n",
    "    Inputs:\n",
    "    - data: the given data\n",
    "    - nsamples: the number of random samples with possible duplicate\n",
    "    - weights: the weights used for AdaBoost\n",
    "\n",
    "    Output:\n",
    "    - the bootstrapped dataset of the random samples with a subset of 3 features\n",
    "    - the random indices with possbile duplicate\n",
    "    \"\"\"\n",
    "    \n",
    "    m = data.shape[1]  # the numbers of columns/ variables\n",
    "    n = data.shape[0]  # the numbers of samples\n",
    "\n",
    "    if nsamples == -1:\n",
    "        rand_sample_indices = np.random.choice(np.arange(n), n, replace=True, p=weights)\n",
    "    else:\n",
    "        rand_sample_indices = np.random.choice(np.arange(n), nsamples, replace=True, p=weights)\n",
    "\n",
    "    # Define the number of subset to be used\n",
    "    num_of_features = 3\n",
    "\n",
    "    # Only use this subset of the features\n",
    "    features_subset = np.random.choice(np.arange(m), num_of_features, replace=False)\n",
    "\n",
    "    # Filter the data based on the random indices and the subset of features\n",
    "    data = data[:, features_subset]\n",
    "    data = data[rand_sample_indices, :]\n",
    "\n",
    "    return data, rand_sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0eeda1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "weights = None\n",
    "p = weights\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
