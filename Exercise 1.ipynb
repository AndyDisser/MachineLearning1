{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6100eb",
   "metadata": {},
   "source": [
    "# Machine Learning 1 - Exercise Sheet 1\n",
    "\n",
    "## Viktor Vironski (4330455), Andy Disser (5984875), Trung (Matrikelnummer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7882956",
   "metadata": {},
   "source": [
    "### Exercise 1. Min-Max normalization\n",
    "\n",
    "Calculate min-max normalization with the following formula for range [a,b]:\n",
    "\n",
    "$$\n",
    "X_{scaled} = a + \\frac{(X - X_{min})*(b - a)}{X_{max} - X_{min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ee743ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a) The first five data point of the feature vectors scaled data set are: \n",
      "\n",
      "[1.         0.67346939 0.24489796 0.        ]\n",
      "[1.         0.59574468 0.25531915 0.        ]\n",
      "[1.         0.66666667 0.24444444 0.        ]\n",
      "[1.         0.65909091 0.29545455 0.        ]\n",
      "[1.         0.70833333 0.25       0.        ]\n",
      "\n",
      "\n",
      "b) The first five data point of the fully scaled data set are: \n",
      "\n",
      "[0.64102564 0.43589744 0.16666667 0.01282051]\n",
      "[0.61538462 0.37179487 0.16666667 0.01282051]\n",
      "[0.58974359 0.3974359  0.15384615 0.01282051]\n",
      "[0.57692308 0.38461538 0.17948718 0.01282051]\n",
      "[0.62820513 0.44871795 0.16666667 0.01282051]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# prevent numpy exponential notation on print \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# implement Min-Max Normalization Function for range 0 to 1\n",
    "def min_max_scale( data: list, min_range: float, max_range: float):\n",
    "    \n",
    "    # copy existing data to a new workable instance\n",
    "    new_data = data.copy()\n",
    "    \n",
    "    # calculate global minimum and global maximum of the data\n",
    "    min_value = np.matrix(new_data).min()\n",
    "    max_value = np.matrix(new_data).max()\n",
    "    \n",
    "    # normalize data using formula x_scaled = (x - min_value)/(max_value - min_value)\n",
    "    new_data -= min_value\n",
    "    new_data *= ( (max_range - min_range) / (max_value - min_value))\n",
    "    new_data += min_range\n",
    "    \n",
    "    # return global minimum, global maximum and the normalized data set\n",
    "    return( new_data, min_value, max_value)\n",
    "\n",
    "\n",
    "# set iris to a numpy array from the iris data\n",
    "iris = np.array(datasets.load_iris().data)\n",
    "\n",
    "### a) scale all features seperatly\n",
    "# initial attempt at transposing the data matrix did not work as planed, therefore loop implementation\n",
    "\n",
    "iris_feature_vectors_normed = []\n",
    "\n",
    "# for all rows in the iris data set apply Min-Max Normalization\n",
    "for i in range(0, len(iris.data)):\n",
    "    iris_feature_vectors_normed.append(min_max_scale(iris[i],0, 1))\n",
    "\n",
    "# print first five data points of the feature normed iris dataset\n",
    "print('a) The first five data points of the feature vectors scaled dataset are:', '\\n')\n",
    "for i in range (0, 5):\n",
    "    print(iris_feature_vectors_normed[i][0])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "### b) scale the full dataset\n",
    "\n",
    "# apply Min-Max Normalization to the full dataset\n",
    "iris_full_dataset_normed = min_max_scale(iris, 0, 1)\n",
    "\n",
    "# print first five data points of the fully scaled dataset\n",
    "print('b) The first five data points of the fully scaled data set are:', '\\n')\n",
    "for j in range (0, 5):\n",
    "    print(iris_full_dataset_normed[0][j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b1adf",
   "metadata": {},
   "source": [
    "### Exercise 2. Z-Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "719397b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a) The first five data points of the feature vectors scaled dataset are: \n",
      "\n",
      "[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]\n",
      "[-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]\n",
      "[-1.38535265  0.32841405 -1.39706395 -1.3154443 ]\n",
      "[-1.50652052  0.09821729 -1.2833891  -1.3154443 ]\n",
      "[-1.02184904  1.24920112 -1.34022653 -1.3154443 ]\n",
      "\n",
      "\n",
      "b) The first five data points of the fully scaled dataset are: \n",
      "\n",
      "[ 0.82858665  0.01798522 -1.04592915 -1.65388022]\n",
      "[ 0.72726147 -0.23532773 -1.04592915 -1.65388022]\n",
      "[ 0.62593629 -0.13400255 -1.09659174 -1.65388022]\n",
      "[ 0.5752737  -0.18466514 -0.99526657 -1.65388022]\n",
      "[ 0.77792406  0.06864781 -1.04592915 -1.65388022]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# prevent numpy exponential notation on print \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# implement Z-Score Normalization\n",
    "def zscore (data: list):\n",
    "    \n",
    "    # copy existing data to a new workable instance\n",
    "    new_data = data.copy()\n",
    "    \n",
    "    # calculate mean\n",
    "    # numpy sum function adds all data in the data set\n",
    "    # numpy size function calculates the size of the matrix aka the number of data points in the dataset\n",
    "    mean = new_data.sum() / new_data.size\n",
    "    \n",
    "    # subtract mean from every data point\n",
    "    new_data -= mean\n",
    "    \n",
    "    # calculate standard diviation\n",
    "    # copy mean calculation matrix into new entity and raise to the power of two\n",
    "    # sum up the standard diviation of all data points and divide by the number of data points\n",
    "    # calculate square root of the result\n",
    "    stan_div_matrix = np.power(new_data.copy(), 2)\n",
    "    stan_div = math.sqrt(stan_div_matrix.sum() / stan_div_matrix.size)\n",
    "    \n",
    "    # normalize data using formula x_new = (x - mean)/stan_div\n",
    "    # (x - mean) has been calculated prior to the calculation of standard diviation, so divide by standard diviation\n",
    "    new_data *= 1/stan_div\n",
    "    \n",
    "    #return Z-Score normalized data\n",
    "    return(new_data)\n",
    "\n",
    "\n",
    "# set iris to a numpy array from the iris data\n",
    "iris = np.array(datasets.load_iris().data)\n",
    "\n",
    "\n",
    "### a) scale variables separately\n",
    "\n",
    "# calculate the Z-Score normalization of every variable seperately\n",
    "# transpose the resulting array to be able to concatenate the four arrays back into a full data set\n",
    "sepal_length = zscore(iris[:, 0:1]).transpose()\n",
    "sepal_width = zscore(iris[:, 1:2]).transpose()\n",
    "petal_length = zscore(iris[:, 2:3]).transpose()\n",
    "petal_width = zscore(iris[:, 3:4]).transpose()\n",
    "\n",
    "# concatenate the four arrays and transpose to acquire the normalized dataset\n",
    "iris_variables_normed = np.concatenate((sepal_length,sepal_width,petal_length,petal_width), axis=0).transpose()\n",
    "\n",
    "# print first five datapoints of the Z-Score normalized iris dataset\n",
    "print('a) The first five data points of the feature vectors scaled dataset are:', '\\n')\n",
    "for i in range (0, 5):\n",
    "    print(iris_variables_normed[i])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#print(iris_variables_scaled)\n",
    "\n",
    "### b) scale the full dataset\n",
    "\n",
    "# apply Z-Score Normalization on iris dataset\n",
    "iris_full_dataset_normed = zscore(iris)\n",
    "\n",
    "# print first five datapoints of the Z-Score normalized iris dataset\n",
    "print('b) The first five data points of the fully scaled dataset are:', '\\n')\n",
    "for j in range(0,5):\n",
    "    print(iris_full_dataset_normed[j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aab536",
   "metadata": {},
   "source": [
    "### Exercise 3. Plotting I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252db817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
